{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guidance by Hugging Face, at https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a\n",
    "Author suggested to use two models:\n",
    "- Forward model: storing the model gradients used in meta forward pass. Parameters gradient populated\n",
    "- Backward model: keeping the parameters as a path for back prop the optimizer gradients. Without gradients updated at the Parameter level to keep track of the computation graph for meta-backward pass\n",
    "Tips:\n",
    "- Use Variable to store the share params and prevent the duplicate memory => update the pointer in the Variable class to point to the same Tensor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "data: 1.0\n",
      "requires_grad: True\n",
      "\\grad: None\n",
      "grad_fn: None\n",
      "is_leaf: True\n",
      "\n",
      "y\n",
      "data: 2.0\n",
      "requires_grad: False\n",
      "\\grad: None\n",
      "grad_fn: None\n",
      "is_leaf: True\n",
      "\n",
      "z\n",
      "data: 2.0\n",
      "requires_grad: True\n",
      "\\grad: None\n",
      "grad_fn: <MulBackward0 object at 0x7fb4f8d5f910>\n",
      "is_leaf: False\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yr/0zzfsy4x4p96lr5wt4klztz40000gn/T/ipykernel_3482/1394791152.py:11: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  print(f\"{name}\\ndata: {i.data}\\nrequires_grad: {i.requires_grad}\\n\\grad: {i.grad}\\ngrad_fn: {i.grad_fn}\\nis_leaf: {i.is_leaf}\\n\")\n"
     ]
    }
   ],
   "source": [
    "# Understanding Pytorch Autograd \n",
    "import torch\n",
    "\n",
    "# Creating the graph\n",
    "x = torch.tensor(1.0, requires_grad = True)\n",
    "y = torch.tensor(2.0)\n",
    "z = x * y\n",
    "\n",
    "# Displaying\n",
    "for i, name in zip([x, y, z], \"xyz\"):\n",
    "    print(f\"{name}\\ndata: {i.data}\\nrequires_grad: {i.requires_grad}\\n\\grad: {i.grad}\\ngrad_fn: {i.grad_fn}\\nis_leaf: {i.is_leaf}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Creating the graph\n",
    "x = torch.tensor(1.0, requires_grad = True)\n",
    "z = x ** 3\n",
    "z.backward() #Computes the gradient \n",
    "print(x.grad.data) #Prints '3' which is dz/dx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d18aaf52573fe1f622460a70d690239c441f3bc32cf3ec35188fdd8fc8b16c6e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
